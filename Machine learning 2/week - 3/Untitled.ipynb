{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03816abf-1bae-4a01-b341-754ecd45f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File IDs in Brown Corpus: ['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12', 'cd13', 'cd14', 'cd15', 'cd16', 'cd17', 'ce01', 'ce02', 'ce03', 'ce04', 'ce05', 'ce06', 'ce07', 'ce08', 'ce09', 'ce10', 'ce11', 'ce12', 'ce13', 'ce14', 'ce15', 'ce16', 'ce17', 'ce18', 'ce19', 'ce20', 'ce21', 'ce22', 'ce23', 'ce24', 'ce25', 'ce26', 'ce27', 'ce28', 'ce29', 'ce30', 'ce31', 'ce32', 'ce33', 'ce34', 'ce35', 'ce36', 'cf01', 'cf02', 'cf03', 'cf04', 'cf05', 'cf06', 'cf07', 'cf08', 'cf09', 'cf10', 'cf11', 'cf12', 'cf13', 'cf14', 'cf15', 'cf16', 'cf17', 'cf18', 'cf19', 'cf20', 'cf21', 'cf22', 'cf23', 'cf24', 'cf25', 'cf26', 'cf27', 'cf28', 'cf29', 'cf30', 'cf31', 'cf32', 'cf33', 'cf34', 'cf35', 'cf36', 'cf37', 'cf38', 'cf39', 'cf40', 'cf41', 'cf42', 'cf43', 'cf44', 'cf45', 'cf46', 'cf47', 'cf48', 'cg01', 'cg02', 'cg03', 'cg04', 'cg05', 'cg06', 'cg07', 'cg08', 'cg09', 'cg10', 'cg11', 'cg12', 'cg13', 'cg14', 'cg15', 'cg16', 'cg17', 'cg18', 'cg19', 'cg20', 'cg21', 'cg22', 'cg23', 'cg24', 'cg25', 'cg26', 'cg27', 'cg28', 'cg29', 'cg30', 'cg31', 'cg32', 'cg33', 'cg34', 'cg35', 'cg36', 'cg37', 'cg38', 'cg39', 'cg40', 'cg41', 'cg42', 'cg43', 'cg44', 'cg45', 'cg46', 'cg47', 'cg48', 'cg49', 'cg50', 'cg51', 'cg52', 'cg53', 'cg54', 'cg55', 'cg56', 'cg57', 'cg58', 'cg59', 'cg60', 'cg61', 'cg62', 'cg63', 'cg64', 'cg65', 'cg66', 'cg67', 'cg68', 'cg69', 'cg70', 'cg71', 'cg72', 'cg73', 'cg74', 'cg75', 'ch01', 'ch02', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch16', 'ch17', 'ch18', 'ch19', 'ch20', 'ch21', 'ch22', 'ch23', 'ch24', 'ch25', 'ch26', 'ch27', 'ch28', 'ch29', 'ch30', 'cj01', 'cj02', 'cj03', 'cj04', 'cj05', 'cj06', 'cj07', 'cj08', 'cj09', 'cj10', 'cj11', 'cj12', 'cj13', 'cj14', 'cj15', 'cj16', 'cj17', 'cj18', 'cj19', 'cj20', 'cj21', 'cj22', 'cj23', 'cj24', 'cj25', 'cj26', 'cj27', 'cj28', 'cj29', 'cj30', 'cj31', 'cj32', 'cj33', 'cj34', 'cj35', 'cj36', 'cj37', 'cj38', 'cj39', 'cj40', 'cj41', 'cj42', 'cj43', 'cj44', 'cj45', 'cj46', 'cj47', 'cj48', 'cj49', 'cj50', 'cj51', 'cj52', 'cj53', 'cj54', 'cj55', 'cj56', 'cj57', 'cj58', 'cj59', 'cj60', 'cj61', 'cj62', 'cj63', 'cj64', 'cj65', 'cj66', 'cj67', 'cj68', 'cj69', 'cj70', 'cj71', 'cj72', 'cj73', 'cj74', 'cj75', 'cj76', 'cj77', 'cj78', 'cj79', 'cj80', 'ck01', 'ck02', 'ck03', 'ck04', 'ck05', 'ck06', 'ck07', 'ck08', 'ck09', 'ck10', 'ck11', 'ck12', 'ck13', 'ck14', 'ck15', 'ck16', 'ck17', 'ck18', 'ck19', 'ck20', 'ck21', 'ck22', 'ck23', 'ck24', 'ck25', 'ck26', 'ck27', 'ck28', 'ck29', 'cl01', 'cl02', 'cl03', 'cl04', 'cl05', 'cl06', 'cl07', 'cl08', 'cl09', 'cl10', 'cl11', 'cl12', 'cl13', 'cl14', 'cl15', 'cl16', 'cl17', 'cl18', 'cl19', 'cl20', 'cl21', 'cl22', 'cl23', 'cl24', 'cm01', 'cm02', 'cm03', 'cm04', 'cm05', 'cm06', 'cn01', 'cn02', 'cn03', 'cn04', 'cn05', 'cn06', 'cn07', 'cn08', 'cn09', 'cn10', 'cn11', 'cn12', 'cn13', 'cn14', 'cn15', 'cn16', 'cn17', 'cn18', 'cn19', 'cn20', 'cn21', 'cn22', 'cn23', 'cn24', 'cn25', 'cn26', 'cn27', 'cn28', 'cn29', 'cp01', 'cp02', 'cp03', 'cp04', 'cp05', 'cp06', 'cp07', 'cp08', 'cp09', 'cp10', 'cp11', 'cp12', 'cp13', 'cp14', 'cp15', 'cp16', 'cp17', 'cp18', 'cp19', 'cp20', 'cp21', 'cp22', 'cp23', 'cp24', 'cp25', 'cp26', 'cp27', 'cp28', 'cp29', 'cr01', 'cr02', 'cr03', 'cr04', 'cr05', 'cr06', 'cr07', 'cr08', 'cr09']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')  # Download the Brown Corpus\n",
    "\n",
    "# Access the Brown Corpus\n",
    "brown_corpus = nltk.corpus.brown\n",
    "\n",
    "# Now you can use the corpus reader methods to read data from the Brown Corpus\n",
    "# For example:\n",
    "print(\"File IDs in Brown Corpus:\", brown_corpus.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0103d135-f52c-45f7-b6a4-7b062dece27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64dfa97e-f1c8-4424-838f-cedb67246b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5417edb-eae4-411f-a245-3266fb9fba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Load the data from the Brown corpus\n",
    "data = brown.raw(brown.fileids()[1])\n",
    "\n",
    "# Tokenize the data into words\n",
    "words = brown.words(brown.fileids()[1])\n",
    "\n",
    "# Convert words to lowercase\n",
    "lowercase_words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c41725f-7486-45c5-ba0c-067d76fde0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words after removing stopwords: 1297\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords and punctuations\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in lowercase_words if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "# Calculate the total number of words after removing stopwords\n",
    "total_words_after_removal = len(filtered_words)\n",
    "print(\"Total words after removing stopwords:\", total_words_after_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0fb10cd-626e-4ea9-9142-5c287a6fe200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top three most frequent words: [('would', 26), ('texas', 21), ('bill', 20)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Load the data from the Brown corpus\n",
    "data = brown.raw(brown.fileids()[1])\n",
    "\n",
    "# Tokenize the data into words\n",
    "words = brown.words(brown.fileids()[1])\n",
    "\n",
    "# Convert words to lowercase\n",
    "lowercase_words = [word.lower() for word in words]\n",
    "\n",
    "# Remove stopwords and punctuations\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in lowercase_words if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "# Get the top three most frequent words\n",
    "top_words = word_freq.most_common(3)\n",
    "print(\"Top three most frequent words:\", top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b26311d-3a3b-47a8-b0df-f57ce8ef36f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigrams: 9\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Given sentence\n",
    "sentence = \"#Great - Learning is the best institute * to learn @data _ science.\"\n",
    "\n",
    "# Clean the sentence by removing special characters\n",
    "cleaned_sentence = re.sub(r\"[#*@_-]\", \"\", sentence)\n",
    "\n",
    "# Tokenize the cleaned sentence into words\n",
    "words = cleaned_sentence.split()\n",
    "\n",
    "# Generate bigrams\n",
    "bigram_list = list(bigrams(words))\n",
    "\n",
    "# Calculate the total number of bigrams\n",
    "num_bigrams = len(bigram_list)\n",
    "print(\"Number of bigrams:\", num_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf00e3a3-0285-4a06-8d99-3fc4250c6f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words after removing stopwords: 1297\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Load the data from the Brown corpus\n",
    "data = brown.raw(brown.fileids()[1])\n",
    "\n",
    "# Tokenize the data into words\n",
    "words = brown.words(brown.fileids()[1])\n",
    "\n",
    "# Convert words to lowercase\n",
    "lowercase_words = [word.lower() for word in words]\n",
    "\n",
    "# Remove stopwords and punctuations\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in lowercase_words if word not in stop_words and word not in string.punctuation]\n",
    "\n",
    "# Calculate the total number of words after removing stopwords\n",
    "total_words_after_removal = len(filtered_words)\n",
    "print(\"Total words after removing stopwords:\", total_words_after_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6960d-37e3-4aee-b9be-16edb7afa156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
